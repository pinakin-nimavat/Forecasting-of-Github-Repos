'''
Goal of LSTM microservice:
1. LSTM microservice will accept the GitHub data from Flask microservice and will forecast the data for next 1 year based on past 30 days
2. It will also plot three different graph (i.e.  "Model Loss", "LSTM Generated Data", "All Issues Data") using matplot lib
3. This graph will be stored as image in Google Cloud Storage.
4. The image URL are then returned back to Flask microservice.
'''
# Import all the required packages
from flask import Flask, jsonify, request, make_response
import os
from dateutil import *
from datetime import timedelta
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import time
from flask_cors import CORS

# Tensorflow (Keras & LSTM) related packages
import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import Input, Dense, LSTM, Dropout
from tensorflow.python.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

# Import required storage package from Google Cloud Storage
from google.cloud import storage

# Initilize flask app
app = Flask(__name__)
# Handles CORS (cross-origin resource sharing)
CORS(app)
# Initlize Google cloud storage client
client = storage.Client()

# DO NOT DELETE "static/images" FOLDER as it is used to store figures/images generated by matplotlib
LOCAL_IMAGE_PATH = "static/images/"

# Add your unique Bucket Name if you want to run it local
BUCKET_NAME = os.environ.get(
    'BUCKET_NAME', 'BUCKET_NAME')

# Add response headers to accept all types of  requests


def build_preflight_response():
    response = make_response()
    response.headers.add("Access-Control-Allow-Origin", "*")
    response.headers.add("Access-Control-Allow-Headers", "Content-Type")
    response.headers.add("Access-Control-Allow-Methods",
                         "PUT, GET, POST, DELETE, OPTIONS")
    return response

#  Modify response headers when returning to the origin


def build_actual_response(response):
    response.headers.set("Access-Control-Allow-Origin", "*")
    response.headers.set("Access-Control-Allow-Methods",
                         "PUT, GET, POST, DELETE, OPTIONS")
    return response


def upload_images(MODEL_LOSS_IMAGE_NAME, ALL_ISSUES_DATA_IMAGE_NAME, LSTM_GENERATED_IMAGE_NAME):
    # Uploads an images into the google cloud storage bucket
    bucket = client.get_bucket(BUCKET_NAME)
    new_blob = bucket.blob(MODEL_LOSS_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + MODEL_LOSS_IMAGE_NAME)
    new_blob = bucket.blob(ALL_ISSUES_DATA_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + ALL_ISSUES_DATA_IMAGE_NAME)
    new_blob = bucket.blob(LSTM_GENERATED_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + LSTM_GENERATED_IMAGE_NAME)
    [os.remove(f'{LOCAL_IMAGE_PATH}{file}') for file in os.listdir(
        f'{LOCAL_IMAGE_PATH}') if file.endswith('.png')]


def generate_url(type, repo_name):
    '''
    Creating image URL
    BASE_IMAGE_PATH refers to Google Cloud Storage Bucket URL.Add your Base Image Path in line 145
    if you want to run the application local
    LOCAL_IMAGE_PATH refers local directory where the figures generated by matplotlib are stored
    These locally stored images will then be uploaded to Google Cloud Storage
    '''
    BASE_IMAGE_PATH = os.environ.get(
        'BASE_IMAGE_PATH', 'BASE_IMAGE_PATH')

    # Creating the image path for model loss, LSTM generated image and all issues data image
    MODEL_LOSS_IMAGE_NAME = "model_loss_lstm_" + type + \
        "_" + repo_name + ".png"
    MODEL_LOSS_URL = BASE_IMAGE_PATH + MODEL_LOSS_IMAGE_NAME

    LSTM_GENERATED_IMAGE_NAME = "lstm_generated_data_lstm_" + \
        type + "_" + repo_name + ".png"
    LSTM_GENERATED_URL = BASE_IMAGE_PATH + LSTM_GENERATED_IMAGE_NAME

    ALL_ISSUES_DATA_IMAGE_NAME = "all_issues_data_lstm_" + \
        type + "_" + repo_name + ".png"
    ALL_ISSUES_DATA_URL = BASE_IMAGE_PATH + ALL_ISSUES_DATA_IMAGE_NAME

    return [
        MODEL_LOSS_IMAGE_NAME,
        MODEL_LOSS_URL,
        LSTM_GENERATED_IMAGE_NAME,
        LSTM_GENERATED_URL,
        ALL_ISSUES_DATA_IMAGE_NAME,
        ALL_ISSUES_DATA_URL
    ]


def perform_forecasting(df, repo_name, type):
    print(f'{repo_name} - df len - {len(df)}')
    array = df.to_numpy()
    x = np.array([time.mktime(i[0].timetuple()) for i in array])
    y = np.array([i[1] for i in array])

    lzip = lambda *x: list(zip(*x))

    days = df.groupby('ds')['ds'].value_counts()
    print(f'{repo_name} - days - {len(days)}')
    Y = df['y'].values
    print(f'{repo_name} - Y - {len(Y)}')
    X = lzip(*days.index.values)[0]
    print(f'{repo_name} - X - {len(X)}')
    firstDay = min(X)
    print(
        f'first day {firstDay} - last day ${max(X)}')

    # To achieve data consistancy with both actual data and predicted values, I'm adding zeros to dates that do not have orders
    # [firstDay + timedelta(days=day) for day in range((max(X) - firstDay).days + 1)]
    Ys = [0, ]*((max(X) - firstDay).days + 1)
    print(f'{repo_name} - Ys len before - {len(Ys)}')
    days = pd.Series([firstDay + timedelta(days=i) for i in range(len(Ys))])
    for x, y in zip(X, Y):
        Ys[(x - firstDay).days] = y

    # modify the data that is suitable for LSTM
    Ys = np.array(Ys)
    Ys = Ys.astype('float32')
    Ys = np.reshape(Ys, (-1, 1))
    scaler = MinMaxScaler(feature_range=(0, 1))
    Ys = scaler.fit_transform(Ys)
    print(f'{repo_name} - Ys len after - {len(Ys)}')
    train_size = int(len(Ys) * 0.80)
    test_size = len(Ys) - train_size
    train, test = Ys[0:train_size, :], Ys[train_size:len(Ys), :]
    print('train size:', len(train), ", test size:", len(test))

    def create_dataset(dataset, look_back=1):
        X, Y = [], []
        for i in range(len(dataset)-look_back-1):
            a = dataset[i:(i+look_back), 0]
            X.append(a)
            Y.append(dataset[i + look_back, 0])
        return np.array(X), np.array(Y)

    # Look back decides how many days of data the model looks at for prediction
    look_back = 30  # Here LSTM looks at approximately one month data
    X_train, Y_train = create_dataset(train, look_back)
    X_test, Y_test = create_dataset(test, look_back)

    # reshape input to be [samples, time steps, features]
    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

    # verifying the shapes
    X_train.shape, X_test.shape, Y_train.shape, Y_test.shape

    # # Model to forecast orders for all zip code
    model = Sequential()
    model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dropout(0.2))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')

    history = model.fit(X_train, Y_train, epochs=20, batch_size=70, validation_data=(X_test, Y_test),
                        callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)

    [MODEL_LOSS_IMAGE_NAME,
     MODEL_LOSS_URL,
     LSTM_GENERATED_IMAGE_NAME,
     LSTM_GENERATED_URL,
     ALL_ISSUES_DATA_IMAGE_NAME,
     ALL_ISSUES_DATA_URL] = generate_url(type, repo_name)

    # model.summary()

    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Test Loss')
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epochs')
    plt.legend(loc='upper right')
    plt.savefig(LOCAL_IMAGE_PATH + MODEL_LOSS_IMAGE_NAME)

    # predict issues for test data
    y_pred = model.predict(X_test)

    fig, axs = plt.subplots(1, 1, figsize=(20, 8))
    X = mdates.date2num(days)
    axs.plot(np.arange(0, len(Y_train)), Y_train, 'g', label="history")
    axs.plot(np.arange(len(Y_train), len(Y_train) + len(Y_test)),
             Y_test, marker='.', label="true")
    axs.plot(np.arange(len(Y_train), len(Y_train) + len(Y_test)),
             y_pred, 'r', label="prediction")
    axs.legend()
    axs.set_title('LSTM generated data')
    axs.set_xlabel('Time steps')
    axs.set_ylabel('Issues')
    plt.savefig(LOCAL_IMAGE_PATH + LSTM_GENERATED_IMAGE_NAME)

    fig, axs = plt.subplots(1, 1, figsize=(20, 8))
    X = mdates.date2num(days)
    axs.plot(X, Ys, 'purple', marker='.')
    locator = mdates.AutoDateLocator()
    axs.xaxis.set_major_locator(locator)
    axs.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))
    axs.legend()
    axs.set_title('All Issues data')
    axs.set_xlabel('Date')
    axs.set_ylabel('Issues')
    plt.savefig(LOCAL_IMAGE_PATH + ALL_ISSUES_DATA_IMAGE_NAME)

    upload_images(MODEL_LOSS_IMAGE_NAME, LSTM_GENERATED_IMAGE_NAME,
                  ALL_ISSUES_DATA_IMAGE_NAME)

    # Construct the response
    json_response = {
        "model_loss_image_url": MODEL_LOSS_URL,
        "lstm_generated_image_url": LSTM_GENERATED_URL,
        "all_issues_data_image": ALL_ISSUES_DATA_URL
    }

    return json_response


'''
API route path is  "/api/forecast"
This API will accept only POST request
'''


@app.route('/api/lstm/forecast', methods=['POST'])
def forecast():
    body = request.get_json()
    created_at_issues = body["created_at_issues"]
    closed_at_issues = body["closed_at_issues"]
    pull_requests = body["pull_requests"]
    commits = body["commits"]
    releases = body["releases"]
    branches = body["branches"]
    repo_name = body["repo"]

    # Created At
    type = "created_at"
    data_frame = pd.DataFrame(created_at_issues)
    df = data_frame.groupby([type], as_index=False).count()
    df = df[[type, 'issue_number']]
    df.columns = ['ds', 'y']
    df['ds'] = df['ds'].astype('datetime64[ns]')
    created_at_response = perform_forecasting(df, repo_name, type)

    # Created At
    type = "closed_at"
    data_frame = pd.DataFrame(created_at_issues)
    df = data_frame.groupby([type], as_index=False).count()
    df = df[[type, 'issue_number']]
    df.columns = ['ds', 'y']
    df['ds'] = df['ds'].astype('datetime64[ns]')
    closed_at_response = perform_forecasting(df, repo_name, type)

    # Pull Requests
    type = "pull_requests"
    df = pd.DataFrame(list(pull_requests.items()))
    df.columns = ['ds', 'y']
    df["ds"] = pd.to_datetime(df["ds"])
    df["ds"] = pd.Series([val.date() for val in df["ds"]])
    df = df.dropna()
    pull_requests_response = perform_forecasting(df, repo_name, type)

    # Commits
    type = "commits"
    df = pd.DataFrame(list(commits.items()))
    df.columns = ['ds', 'y']
    df["ds"] = pd.to_datetime(df["ds"])
    df["ds"] = pd.Series([val.date() for val in df["ds"]])
    df = df.dropna()
    commits_response = perform_forecasting(df, repo_name, type)

    # Releases
    type = "releases"
    df = pd.DataFrame(list(releases.items()))
    df.columns = ['ds', 'y']
    df["ds"] = pd.to_datetime(df["ds"])
    df["ds"] = pd.Series([val.date() for val in df["ds"]])
    df = df.dropna()
    releases_response = perform_forecasting(df, repo_name, type)

    # Branches
    type = "branches"
    df = pd.DataFrame(list(branches.items()))
    df.columns = ['ds', 'y']
    df["ds"] = pd.to_datetime(df["ds"])
    df["ds"] = pd.Series([val.date() for val in df["ds"]])
    df = df.dropna()
    branches_response = perform_forecasting(df, repo_name, type)

    # Construct the response
    json_response = {
        "createdAtImageUrls": {
            **created_at_response
        },
        "closedAtImageUrls": {
            **closed_at_response
        },
        "pullRequestsImageUrls": {
            **pull_requests_response
        },
        "commitsImageUrls": {
            **commits_response
        },
        "releasesImageUrls": {
            **releases_response
        },
        "branchesImageUrls": {
            **branches_response
        }
    }
    # Returns image url back to flask microservice
    return jsonify(json_response)


@app.route('/api/forecast/contributors', methods=['POST'])
def forecast_contributors():
    body = request.get_json()
    contributors = body["contributors"]
    repo_name = body["repo"]
    type = body["type"]

    df = pd.DataFrame(list(contributors.items()))
    df.columns = ['ds', 'y']
    # df["ds"] = pd.to_datetime(df["ds"])
    # df["ds"] = pd.Series([val.date() for val in df["ds"]])
    df = df.dropna()
    json_response = perform_forecasting(df, repo_name, type)
    return jsonify(json_response)


# Run LSTM app server on port 8080
if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=8080)
